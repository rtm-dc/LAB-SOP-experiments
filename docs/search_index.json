[["index.html", "SOPs for The Lab @ DC Preface", " SOPs for The Lab @ DC Ryan T. Moore 2023-01-24 Preface This is the very first part of the book. "],["getting-started.html", "Chapter 1 Getting Started", " Chapter 1 Getting Started Before the design of the experiment Copy the Asana project template for your project Set up GitHub repository for the study Set up Google Drive folders for the study See the Asana project template for more The pre-analysis plan template lives at "],["design.html", "Chapter 2 Design 2.1 Units 2.2 Level of Randomization 2.3 Blocking on Background Characteristics 2.4 Setting the Assignment Seed 2.5 Power 2.6 Balance Checking", " Chapter 2 Design 2.1 Units We carefully define the units of observation. They maybe households, automobile registrations, 911 callers, or students, about which we are measuring application status, traffic tickets, primary care visits, or days of attendance in school, respectively. 2.2 Level of Randomization We define the level of randomization, which is not always the same as the unit of observation. In particular, the units of observation may be assigned in clusters. For example, we may assign some classrooms to a particular teacher-based intervention, and all students in intervention classrooms are assigned to the same intervention. When clusters are meaningful, this has substantial implications for our design and analysis. Generally, we prefer to randomize at lower levels of aggregation – at the student level rather than the classroom level, e.g. – because we will have more randomization units. However, there are often logistical or statistical reasons for assigning conditions in clusters. For example, logistically, a teacher can only deliver one particular curriculum to their class; statistically, we may be concerned about interference between students, where students in one condition interact with students in the other, and causal effects of the intervention are difficult to isolate. 2.3 Blocking on Background Characteristics In order to create balance on potential outcomes, which promotes less estimation error and more precision, we block using prognostic covariates. A blocked randomization first creates groups of similar randomization units, and then randomizes within those groups. In a random allocation, by contrast, one assigns a proportion of units to each treatment condition from a single pool. See Moore (2012) for discussion. 2.3.1 Examples The Lab’s TANF recertification experiment (Moore et al. 2022) blocked participants on service center and assigned visit date. 2.4 Setting the Assignment Seed Whenever our design or analysis involves randomization, we set the seed so that our results can be perfectly replicated.1 To set the random seed, run at the R prompt sample(1e9, 1) then copy and paste the result as the argument of set.seed(). If the result of sample(1e9, 1) is SSSSSSSSS, then put set.seed(SSSSSSSSS) at the top of the .R file, just after the library() commands that load and attach that file’s packages. In a short random assignment file, e.g., we might have # Packages: library(dplyr) # Set seed: set.seed(SSSSSSSSS) # Conduct Bernoulli randomization: df &lt;- df %&gt;% mutate( treatment = sample(0:1, size = n(), replace = TRUE)) 2.5 Power We conduct power analysis to determine how precise our experiment are likely to be. Power analysis should go beyond sample size, and attempt to account for as many features of the design, implementation, and analysis as possible. Sometimes we can achieve this with “formula-based” power strategies; other times we need to use simulation-based techniques. Power analyses should be conducted in code, so that they are replicable. If we use an online calculator for a quick calculation, we replicate the results in code.2 If our design and analysis plan match the assumptions of a formula-based power calculation well, we perform a formula-based power calculation. For example, if the design is complete randomization and the analysis plan is to conduct a two-sample \\(t\\)-test, we might use R’s power.t.test(), as below. However, if the design includes blocked assignment in several waves, untreated units stay in the pool across waves, and assignment probabilities vary by wave, with an analysis plan of covariance-adjusted Lin estimation with strong covariates, then we need to use simulation. If we can’t find a formula-based approach that sufficiently approximates our design and analysis plan, we use simulation. 2.5.1 Formula-based Power Analysis An example of formula-based power calculation, where the design is complete randomization and the analysis plans for a two-sample \\(t\\)-test: power_out &lt;- power.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8) power_out ## ## Two-sample t test power calculation ## ## n = 16.71477 ## delta = 1 ## sd = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group For two equally-sized samples drawn from a population with standard normal outcomes, we need 17 observations in each group to have a probability of 0.8 of detecting a true effect that is one standard deviation of the outcome in size, where “detecting” means rejecting a null hypothesis of \\(H_0: \\mu_{Tr} = \\mu_{Co}\\) against an alternative of \\(H_a: \\mu_{Tr} \\neq \\mu_{Co}\\) using \\(\\alpha = 0.05\\). 2.5.2 Simulation-based Power Analysis Simulation-based power analysis allows us to estimate the power of any combination of randomization technique, missing data treatment, estimation strategy, etc. that we like. Create some data to illustrate simulation-based power analysis: library(estimatr) library(here) library(tidyverse) set.seed(988869862) n_samp &lt;- 100 df &lt;- tibble(x = rchisq(n_samp, df = 3), z = rbinom(n_samp, 1, prob = 0.5), y = x + z + rnorm(n_samp, sd = 1.1)) save(df, file = here(&quot;data&quot;, &quot;02-01-df.RData&quot;)) Suppose the estimation strategy is linear regression \\(y_i = \\beta_0 + \\beta_1 z_i + \\beta_2 x_i + \\epsilon_i\\) with heteroskedasticity-robust HC2 standard errors, and the coefficient of interest is \\(\\beta_1\\). Perform 1000 reassignments and determine what proportion of them reveal \\(\\hat{\\beta}_1\\) that is statistically significant at \\(\\alpha = 0.05\\). n_sims &lt;- 1000 alpha &lt;- 0.05 true_te &lt;- 1 is_stat_sig &lt;- vector(&quot;logical&quot;, n_sims) # Storage for(idx in 1:n_sims){ # Re-assign treatment and recalculate outcomes n_sims times: # (Note: conditions on original x in df, but not original y.) df &lt;- df %&gt;% mutate(z_tmp = rbinom(n_samp, 1, prob = 0.5), y_tmp = true_te * z_tmp + x + rnorm(100, sd = 1.1)) # Estimation: lm_out &lt;- lm_robust(y_tmp ~ z_tmp + x, data = df) # Store p-value: stat_sig_tmp &lt;- lm_out$p.value[&quot;z_tmp&quot;] # Store whether true effect is &#39;detected&#39;: is_stat_sig[idx] &lt;- (stat_sig_tmp &lt;= alpha) } mean(is_stat_sig) ## [1] 0.995 So the probability of detecting the true average treatment effect of 1 is about 0.995. This is high power comes largely from the strongly predictive nature of the covariate x. Note that a naïve formula-based approach that ignores the data generating process estimates the power to be roughly 0.45. 2.6 Balance Checking To increase our certainty that our treatment and control conditions are balanced on predictive covariates, we compare the distributions of covariates. For example, in Moore et al. (2022), we describe the median absolute deviation (MAD) of appointment dates is about 0.15 days (about 3.5 hours) or less in 99% of the trios. In other words, the medians of the treatment and control groups tend to vary by much less than a day across the months and Service Centers. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
