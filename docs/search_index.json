[["index.html", "SOPs for The Lab @ DC Preface Colophon", " SOPs for The Lab @ DC Ryan T. Moore 2023-02-21 Preface This provides statistical guidance for randomized experiments, observational causal inference, and data analysis conducted at The Lab @ DC. The appendices provide examples of some fundamental operations in R and some standards for managing code and workflow. These examples intend to serve as templates that you can copy, alter, and use in your project work. Please also see our developing data science standard operating procedures and, perhaps, our older wiki. Colophon This version of the book was built with ## setting value ## version R version 4.2.2 (2022-10-31) ## os macOS Ventura 13.2 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2023-02-21 ## pandoc 2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown) Along with these packages: The code for building this colophon comes from Jenny Bryan’s book here. "],["getting-started.html", "Chapter 1 Getting Started", " Chapter 1 Getting Started Before the design of the experiment Copy the Asana project template for your project Set up GitHub repository for the study Set up Google Drive folders for the study See the Asana project template for more The pre-analysis plan template lives at "],["design.html", "Chapter 2 Design 2.1 Units 2.2 Level of Randomization 2.3 Blocking on Background Characteristics 2.4 Setting the Assignment Seed 2.5 Power 2.6 Balance Checking", " Chapter 2 Design 2.1 Units We carefully define the units of observation. They maybe households, automobile registrations, 911 callers, or students, about which we are measuring application status, traffic tickets, primary care visits, or days of attendance in school, respectively. 2.2 Level of Randomization We define the level of randomization, which is not always the same as the unit of observation. In particular, the units of observation may be assigned in clusters. For example, we may assign some classrooms to a particular teacher-based intervention, and all students in intervention classrooms are assigned to the same intervention. When clusters are meaningful, this has substantial implications for our design and analysis. Generally, we prefer to randomize at lower levels of aggregation – at the student level rather than the classroom level, e.g. – because we will have more randomization units. However, there are often logistical or statistical reasons for assigning conditions in clusters. For example, logistically, a teacher can only deliver one particular curriculum to their class; statistically, we may be concerned about interference between students, where students in one condition interact with students in the other, and causal effects of the intervention are difficult to isolate. 2.3 Blocking on Background Characteristics In order to create balance on potential outcomes, which promotes less estimation error and more precision, we block using prognostic covariates. A blocked randomization first creates groups of similar randomization units, and then randomizes within those groups. In a random allocation, by contrast, one assigns a proportion of units to each treatment condition from a single pool. See Moore (2012) for discussion. 2.3.1 Examples The Lab’s TANF recertification experiment (Moore et al. 2022) blocked participants on service center and assigned visit date. 2.4 Setting the Assignment Seed Whenever our design or analysis involves randomization, we set the seed so that our results can be perfectly replicated.1 To set the random seed, run at the R prompt sample(1e9, 1) then copy and paste the result as the argument of set.seed(). If the result of sample(1e9, 1) is SSSSSSSSS, then put set.seed(SSSSSSSSS) at the top of the .R file, just after the library() commands that load and attach that file’s packages. In a short random assignment file, e.g., we might have # Packages: library(dplyr) # Set seed: set.seed(SSSSSSSSS) # Conduct Bernoulli randomization: df &lt;- df %&gt;% mutate( treatment = sample(0:1, size = n(), replace = TRUE)) 2.5 Power We conduct power analysis to determine how precise our experiment are likely to be. Power analysis should go beyond sample size, and attempt to account for as many features of the design, implementation, and analysis as possible. Sometimes we can achieve this with “formula-based” power strategies; other times we need to use simulation-based techniques. Power analyses should be conducted in code, so that they are replicable. If we use an online calculator for a quick calculation, we replicate the results in code.2 If our design and analysis plan match the assumptions of a formula-based power calculation well, we perform a formula-based power calculation. For example, if the design is complete randomization and the analysis plan is to conduct a two-sample \\(t\\)-test, we might use R’s power.t.test(), as below. However, if the design includes blocked assignment in several waves, untreated units stay in the pool across waves, and assignment probabilities vary by wave, with an analysis plan of covariance-adjusted Lin estimation with strong covariates, then we need to use simulation. If we can’t find a formula-based approach that sufficiently approximates our design and analysis plan, we use simulation. 2.5.1 Formula-based Power Analysis An example of formula-based power calculation, where the design is complete randomization and the analysis plans for a two-sample \\(t\\)-test: power_out &lt;- power.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8) power_out ## ## Two-sample t test power calculation ## ## n = 16.71477 ## delta = 1 ## sd = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group For two equally-sized samples drawn from a population with standard normal outcomes, we need 17 observations in each group to have a probability of 0.8 of detecting a true effect that is one standard deviation of the outcome in size, where “detecting” means rejecting a null hypothesis of \\(H_0: \\mu_{Tr} = \\mu_{Co}\\) against an alternative of \\(H_a: \\mu_{Tr} \\neq \\mu_{Co}\\) using \\(\\alpha = 0.05\\). 2.5.2 Formula-based MDE (minimum detectable effect) An example of a formula-based MDE calculation follows, where the analysis plans for a two-sample test of proportions. The sample size is 75 (in each group), and we want to detect stipulated effects with probability 0.8. Below, we make the most conservative (SE-maximizing) possible assumption about the base rate, that it is 0.5. power_out_mde &lt;- power.prop.test(n = 75, p1 = 0.5, power = 0.8) power_out_mde ## ## Two-sample comparison of proportions power calculation ## ## n = 75 ## p1 = 0.5 ## p2 = 0.7213224 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group power_out_mde$p2 - power_out_mde$p1 ## [1] 0.2213224 We see a minimum detectable effect of about 0.22, over a base rate of 0.5. 2.5.3 Simulation-based Power Analysis Simulation-based power analysis allows us to estimate the power of any combination of randomization technique, missing data treatment, estimation strategy, etc. that we like. Create some data to illustrate simulation-based power analysis: library(estimatr) library(here) library(tidyverse) set.seed(988869862) n_samp &lt;- 100 df &lt;- tibble(x = rchisq(n_samp, df = 3), z = rbinom(n_samp, 1, prob = 0.5), y = x + z + rnorm(n_samp, sd = 1.1)) save(df, file = here(&quot;data&quot;, &quot;02-01-df.RData&quot;)) Suppose the estimation strategy is linear regression \\(y_i = \\beta_0 + \\beta_1 z_i + \\beta_2 x_i + \\epsilon_i\\) with heteroskedasticity-robust HC2 standard errors, and the coefficient of interest is \\(\\beta_1\\). Perform 1000 reassignments and determine what proportion of them reveal \\(\\hat{\\beta}_1\\) that is statistically significant at \\(\\alpha = 0.05\\). n_sims &lt;- 1000 alpha &lt;- 0.05 true_te &lt;- 1 is_stat_sig &lt;- vector(&quot;logical&quot;, n_sims) # Storage for(idx in 1:n_sims){ # Re-assign treatment and recalculate outcomes n_sims times: # (Note: conditions on original x in df, but not original y.) df &lt;- df %&gt;% mutate(z_tmp = rbinom(n_samp, 1, prob = 0.5), y_tmp = true_te * z_tmp + x + rnorm(100, sd = 1.1)) # Estimation: lm_out &lt;- lm_robust(y_tmp ~ z_tmp + x, data = df) # Store p-value: stat_sig_tmp &lt;- lm_out$p.value[&quot;z_tmp&quot;] # Store whether true effect is &#39;detected&#39;: is_stat_sig[idx] &lt;- (stat_sig_tmp &lt;= alpha) } mean(is_stat_sig) ## [1] 0.995 So the probability of detecting the true average treatment effect of 1 is about 0.995. This is high power comes largely from the strongly predictive nature of the covariate x. Note that a naïve formula-based approach that ignores the data generating process estimates the power to be roughly 0.45. 2.6 Balance Checking To increase our certainty that our treatment and control conditions are balanced on predictive covariates, we compare the distributions of covariates. For example, in Moore et al. (2022), we describe the median absolute deviation (MAD) of appointment dates is about 0.15 days (about 3.5 hours) or less in 99% of the trios. In other words, the medians of the treatment and control groups tend to vary by much less than a day across the months and Service Centers. References "],["implementation.html", "Chapter 3 Implementation", " Chapter 3 Implementation Here. "],["analysis.html", "Chapter 4 Analysis 4.1 Notation 4.2 Estimands 4.3 The Unadjusted ITT 4.4 Adjusting for covariates 4.5 Binary or Count Outcomes 4.6 Clusters, weights, fixed effects 4.7 Randomization Inference 4.8 Addressing non-compliance 4.9 Missing Data 4.10 Multiple Comparisons 4.11 Posterior Probabilities", " Chapter 4 Analysis In experiments, our standard operating procedure is to estimate the treatment effect via the unadjusted ITT the adjusted ITT, where we have potentially informative covariates We incorporate features of our design into our analysis. This includes blocking, clustering, and weighting when treatment assignment probabilities vary. By default, we perform parametric statistical inference using HC2 standard errors if our assignment is not clustered. If our assignment is clustered, we use CR2 standard errors with clustering at the level of assignment. Often, however, we specify that we will use design-based randomization inference instead of parametric inference. This design-based inference attempts to replicate all the features of our assignment and analysis. Some times that we rely on randomization inference include when we have a multistage or wave-based design that creates complex assignment probabilities, a skewed outcome3 or other distributional concern, or a novel adjustment strategy. We report treatment effect estimates and associated confidence intervals. We often report the posterior probability that one condition is better than another. 4.1 Notation Notation: \\(n\\) units, indexed \\(i \\in \\{1, \\ldots, n\\}\\) \\(Z_i \\in \\{0, 1\\}\\) indicates assignment to the control or treatment condition \\(D_i \\in \\{0, 1\\}\\) indicates receipt of the control or treatment condition \\(Y_i(1)\\) the potential outcome for unit \\(i\\) if assigned to treatment (more explicitly, \\(Y_i(Z_i = 1)\\)) \\(Y_i(0)\\) the potential outcome for unit \\(i\\) if assigned to control \\(Y_i\\) the observed potential outcome for unit \\(i\\) 4.2 Estimands Below are some estimands (targets) that we may be interested in. Average treatment effect: \\(ATE = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (Y_i(1) - Y_i(0))\\) LATE CACE Effect for those most helped (Ruggeri and Folke 2021) 4.3 The Unadjusted ITT By default, in experiments, we estimate the intent-to-treat effect (ITT), unadjusted for covariates, using HC2 standard errors for inference. We do so by estimating the coefficients of the model \\[Y_i = \\beta_0 + \\beta_1 Z_i + \\epsilon_i\\] using least squares. First, some preliminaries: library(dplyr) library(estimatr) library(here) # Load data: load(here(&quot;data&quot;, &quot;02-01-df.RData&quot;)) Our estimation procedure: lm_out &lt;- lm_robust(y ~ z, data = df) lm_out ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 2.631066 0.2866799 9.177716 7.370994e-15 2.0621596 3.199973 98 ## z 1.548357 0.5367332 2.884779 4.816584e-03 0.4832272 2.613486 98 We can view the ITT treatment effect from the model object with summary(lm_out) or extract it with lm_out$coefficients[\"z\"]. 4.4 Adjusting for covariates In analyzing experiments, we follow the guidance of Lin (2013), using HC2 standard errors for inference, and, where we adjust for covariates, centering and interacting predictors with treatment status. Where \\(Z\\) is treatment status and \\(X\\) is a covariate, \\(\\beta_1\\) below is the ATE: \\[y_i = \\beta_0 + \\beta_1 Z_i + \\beta_2 (X_i - \\bar{X}) + \\beta_3 Z_i (X_i - \\bar{X}) + \\epsilon_i\\] To implement this, we can use lm_lin() from the estimatr package4: lin_out &lt;- lm_lin(y ~ z, covariates = ~ x, data = df) lin_out ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 2.8816525 0.15663330 18.39744 3.088795e-33 2.57073780 3.1925671 96 ## z 0.9487427 0.23402169 4.05408 1.023250e-04 0.48421335 1.4132721 96 ## x_c 0.8773304 0.06813581 12.87620 1.239045e-22 0.74208190 1.0125789 96 ## z:x_c 0.1224778 0.08153824 1.50209 1.363561e-01 -0.03937434 0.2843299 96 We can view the adjusted ITT treatment effect from the model object with summary(lin_out) or extract it with lin_out$coefficients[\"z\"]. The estimate of the average treatment effect is \\(0.949\\), with a 95% confidence interval covering \\((0.484, 1.413)\\). 4.5 Binary or Count Outcomes We estimate the linear models above, even when we have binary or count outcomes. The Lin estimator returns a good estimate of the difference in means of non-normal outcomes; Lin (2013) provides an example with a highly skewed outcome, e.g. 4.6 Clusters, weights, fixed effects To specify other design or estimation components for lm_robust() or lm_lin(), start at the help file at https://declaredesign.org/r/estimatr/reference/lm_robust.html. For example, for a clustered assignment analysis, lm_robust(y ~ z, data = df, clusters = cluster_id) Though our standard procedure is to cluster at the level of assignment, we note that “[s]ometimes you need to cluster standard errors above the level of treatment.” 4.6.1 Weights When our units have different probabilities of assignment, we weight each unit by the reciprocal of the probability of being assigned to the condition that the unit is finally assigned to. We use inverse probability weights (IPW).5 For example, suppose we have a monthly lottery from January to March, and those not selected for the program in the first month are still eligible to be selected in the second month. See Avila et al. (2022) for an example. Person \\(A\\) is eligible for all three waves of a lottery because they apply in January and could be repeatedly assigned to the control group; person \\(B\\) applies in March and is only eligible in the last lottery. If the probability of treatment in each wave is \\(p\\), then the probabilities of treatment and control for \\(A\\) and \\(B\\) are Unit Entry \\(Pr(Z = 1)\\) \\(Pr(Z = 0)\\) \\(A\\) January \\(p + (1-p)p + (1-p)^2p\\) \\(1-[p + (1-p)p + (1-p)^2p]\\) \\(B\\) March \\(p\\) \\(1-p\\) and the weights are Unit Entry IPW if \\((Z = 1)\\) IPW if \\(Z = 0\\) \\(A\\) January \\(\\frac{1}{p + (1-p)p + (1-p)^2p}\\) \\(\\frac{1}{1-[p + (1-p)p + (1-p)^2p]}\\) \\(B\\) March \\(\\frac{1}{p}\\) \\(\\frac{1}{1-p}\\) If the assignment probability varies by wave, i.e., \\(p\\) is not constant, then this fact needs to be taken into account. 4.7 Randomization Inference Often, we prefer to rely on our design, rather than asymptotic or distributional assumptions, for statistical inference. For example, with modest sample sizes and a complex experimental design, in Avila et al. (2022) we use randomization inference. Below is an annotated example of doing so, using a for loop. We set the seed using the method in Section 2.4; we simulate treatment assignments using the method in Section A.8. More simulations reduces simulation error. # Set seed: set.seed(534722898) # Set simulation size: n_sims &lt;- 1000 # Create empty storage vector: store_te &lt;- vector(&quot;double&quot;, length = n_sims) # Rerandomise and perform estimation n_sims times: for(i in 1:n_sims){ # Reassign 0/1 treatment: df_tmp &lt;- df %&gt;% mutate(z_tmp = sample(0:1, n(), replace = TRUE)) # Estimate treatment effect: lm_tmp &lt;- lm_robust(y ~ z_tmp, data = df_tmp) # Store estimate: store_te[i] &lt;- lm_tmp$coefficients[&quot;z_tmp&quot;] } # Estimate treatment effect from actual assignment: te_est &lt;- lm_out$coefficients[&quot;z&quot;] # The p-value: # &quot;What proportion of effects estimated under sharp null hypothesis # are at least as extreme as that which we observed?&quot; mean(abs(store_te) &gt;= abs(te_est)) ## [1] 0.003 Sometimes, this will differ greatly from the parametric \\(p\\)-value. Here, the parametric \\(p\\)-value is 0.00482. 4.8 Addressing non-compliance 4.9 Missing Data Unless we have solid justification to believe the missingness mechanism is MCAR (missing completely at random), we prefer to treat missing data with multiple imputation. We recognize that not all missing data are the same, however. We can confidently impute values that we are certain exist, but we should be more circumspect regarding other values. For example, we can confidently impute a household income, but we should be more circumspect about a student’s grade point average (GPA) or a survey preference. How would we interpret that GPA if the student actually dropped out of school? How can we be confident that the survey respondent actually has a preference for alternative \\(A\\) or \\(B\\), rather than being indifferent or having never considered the question? 4.10 Multiple Comparisons In frequentist hypothesis testing, the more tests we conduct, the more likely we become to “detect” a causal effect that isn’t real, but is only an artifact of natural variation and random assignment. However, we are often interested in learning as much as we can from an experiment, such as the effect of an intervention on several outcomes, or the effect in subgroups, or the effect of several conditions. This tension underlies the problem of multiple comparisons. One way to avoid this problem is to use Bayesian multilevel modeling, where post-analysis adjustment to probability statements or uncertainty intervals is usually not necessarily (Gelman, Hill, and Yajima 2012). Another approach is to adjust frequentist \\(p\\)-values. Our default strategy for multiple comparisons is to control the family-wise error rate (FWER). This approach also controls the false discovery rate (FDR), but can be very conservative. We account for multiple tests when a) we declare the tests to be confirmatory, and b) the tests form a family. Otherwise, we do not account for multiple tests. By default, we control the FWER using the Holm-Bonferroni procedure (Holm 1979). With many tests or correlated tests, this adjustment can result in significant losses in power. When we anticipate highly correlated tests, such as testing several outcome measures of the same construct, or are interested in interval estimation, we use a bootstrap resampling procedure (Westfall and Young 1993).6 If we test a single outcome in several time periods, these tests are likely to be highly correlated with each other. For example, in the ATE project, we expect to test whether treatment affected the outcome at 3 months and at 6 months. In this case, we adjust for performing two tests using the bootstrap resampling procedure of Westfall and Young (1993). 4.10.1 Holm-Bonferroni adjustment Suppose we have conducted three confirmatory hypothesis tests in a family, with \\(p\\)-values \\(p_1 = .01\\), \\(p_2 = .02\\), and \\(p_3 = .08\\), in increasing order. To adjust these \\(p\\)-values with the Holm procedure, we use p.adjust(c(0.01, 0.02, 0.08), method = &quot;holm&quot;) ## [1] 0.03 0.04 0.08 We see that the three adjusted \\(p\\)-values are \\(0.03\\), \\(0.04\\), and \\(0.08\\). (Here, these are \\(p_1 \\times 3\\), \\(p_2 \\times 2\\), and \\(p_3 \\times 1\\).) 4.10.2 Westfall-Young adjustment Westfall and Young (1993) provides a bootstrapping approach to \\(p\\)-value adjustment that tends to be more powerful than the Holm-Bonferroni procedure.7 It strongly controls the FWER under the assumption of subset pivotality. We show an example using the implementation of Hidalgo (2017) below. # Add another outcome and centered covariate: df &lt;- df %&gt;% mutate(y2 = 0.5 * z + rnorm(nrow(df), sd = 1), x_c = as.vector(scale(x, scale = FALSE))) # Install multitestr: # devtools::install_github(&quot;fdhidalgo/multitestr&quot;) library(multitestr) # Define formulas with treatment (and interaction): ff &lt;- lapply(list( &quot;y ~ z + x_c + z * x_c&quot;, &quot;y2 ~ z + x_c + z * x_c&quot;), as.formula) # Define null formulas without treatment: ff_null &lt;- lapply(list( &quot;y ~ x_c&quot;, &quot;y2 ~ x_c&quot;), as.formula) wy_out &lt;- boot_stepdown( full_formulas = ff, null_formulas = ff_null, data = df, coef_list = list(coef = c(&quot;z&quot;, &quot;z:x_c&quot;)), nboots = 1000, parallel = FALSE, boot_type = &quot;wild&quot;, pb = FALSE) wy_out %&gt;% mutate(across(where(is.numeric), round, 3)) ## Hypothesis coef bs_pvalues_unadjusted bs_pvalues_adjusted ## 1 Hypothesis 1 z 0.001 0.001 ## 2 Hypothesis 2 z:x_c 0.148 0.268 ## 3 Hypothesis 1 z 0.073 0.247 ## 4 Hypothesis 2 z:x_c 0.180 0.268 4.11 Posterior Probabilities We may be interested in the probability that one treatment condition outperforms another. In Moore et al. (2022), we calculate that open deadlines have probability 0.79 of being better than specific deadlines. These probabilities can provide guidance for agencies about which treatment condition(s) to continue implementing after an experiment ends, especially when there appears to be little difference between conditions in effect or cost. References "],["observational-studies.html", "Chapter 5 Observational Studies 5.1 Matching", " Chapter 5 Observational Studies We occasionally try to estimate causal effects using data from non-randomized, observational sources. When we do so, we seek to embody the principle of “design before analysis” and the techniques in Rubin (2007). 5.1 Matching References "],["publication.html", "Chapter 6 Publication 6.1 Authorship", " Chapter 6 Publication Public version of GitHub repository Academic papers 6.1 Authorship For academic papers and technical reports, we ascribe to the spirit of the definitions Nature lays out here. "],["r-basics.html", "A R Basics A.1 Get R and RStudio A.2 Read in data files A.3 Pipes A.4 Rename variables A.5 Create a New Variable A.6 Recode a variable’s values A.7 Treat dates as dates A.8 Create a completely randomised indicator A.9 Plot a variable A.10 Calculate a data summary A.11 Merge (join) dataframes", " A R Basics A.1 Get R and RStudio First download and install R from https://cran.r-project.org. Then, download and install RStudio (Desktop) from https://posit.co/products/open-source/rstudio/. (You should have permission for both of these. If not, you may need to contact EOM IT.) A.2 Read in data files We can read data from many formats and many places, including from a local file or from the web. Many R packages contain their own data. The code below requires the following packages to be loaded and attached: library(dplyr) library(ggplot2) library(here) library(lubridate) library(magrittr) library(readr) A.2.1 .csv Data from the web (from the résumé audit experiment from Bertrand and Mullainathan (2004)): df_resume &lt;- read_csv(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/resume.csv&quot;) Then we can examine the dimensions of the new object named df_resume: dim(df_resume) ## [1] 4870 4 So, df_resume has 4870 rows and 4 columns. We can see the first few rows with head(df_resume) ## # A tibble: 6 × 4 ## firstname sex race call ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Allison female white 0 ## 2 Kristen female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 ## 5 Carrie female white 0 ## 6 Jay male white 0 For data from a local file, if data mydata.csv are in subdirectory /data/, my_data_path &lt;- here(&quot;data&quot;, &quot;mydata.csv&quot;) df &lt;- read_csv(my_data_path) See here for more detail on how to use here() to create paths. A.2.2 .xlsx Use package readxl and function read_excel(). To read the second sheet from a multi-sheet spreadsheet, add an argument like read_excel(..., sheet = 2). A.2.3 Box To read a file directly from box, we use the boxr package. To do so, Have Nathan add you as Box RStudio App collaborator Go to dcgov.box.com. At the bottom left, click “Dev Console” Click on RStudio Access Click the Configuration tab Here you should see a client_id and client_secret that you can copy. Then, run at the R Console library(boxr) box_auth(client_id = &quot;&lt;INSERT ID STRING HERE&gt;&quot;, client_secret = &quot;&lt;INSERT SECRET STRING HERE&gt;&quot;) replacing the strings with the information from the Configuration tab. If R prompts you to update the Renviron file to store this information for next time, you may do so. Then, your .R file that reads the data should include library(boxr) box_auth() # (arguments can be empty, if credentials stored in Renviron) df &lt;- box_read(&quot;791112121820&quot;) # (where 791112121820 is the Box ID for a .csv to read) For more information, see the boxr vignette. A.3 Pipes Pipe operators make it easier for humans to follow a code chunk, and they allow us to string together a sequence of operations into a “pipeline”. The standard pipe from package magrittr is most common (%&gt;%), but there are others, including a native R pipe (|&gt;). The pipe simply takes the argument on its left and passes it as the first argument of the function to its right. So, if I have function f() to which I will pass arguments x and y (in that order), I can write f(x, y) Using a pipe, I can equivalently write either x %&gt;% f(y) or x |&gt; f(y) A.4 Rename variables Variable names should adhere to good practices. The rename() function in dplyr can rename several variables at once: df &lt;- df %&gt;% rename(new_var_name_1 = `Bad, old, long, spacey name`, new_var_name_2 = &quot;inconsistentsquishedvarname&quot;, new_var_name_3 = starts_with(&quot;Unique string old name starts with&quot;)) To automatically clean up all the names in a dataframe, the janitor package can help: df &lt;- df %&gt;% clean_names() A.5 Create a New Variable Suppose I want to create a new variable and attach it to the data frame (i.e., to update the data frame to include the new variable). Mechanically, in the tidyverse I overwrite the old data frame with the new one. Sometimes we want to use a new name for the augmented data frame, but not when we create each new variable. A.5.1 Create a “wave ID” A “wave ID” is a variable that is constant for every row in the data set. Suppose that the resume data were all from “wave 3” of a larger study. To identify them as such, df_resume &lt;- df_resume %&gt;% mutate(wave = 3) A.5.2 Create a primary key A “primary key” is a variable that uniquely identifies each row in the data set. To create a primary key or “ID variable”, df_resume &lt;- df_resume %&gt;% mutate(id_var = row_number()) A.5.3 Transform an existing variable Below, we use the mutate() function with the character variable sex, which has levels female and male, to create a logical TRUE/FALSE variable indicating whether the résumé had a putatively female name at the top: df_resume &lt;- df_resume %&gt;% mutate(isFemale = sex == &quot;female&quot;) Now, df_resume has 7 columns: head(df_resume) ## # A tibble: 6 × 7 ## firstname sex race call wave id_var isFemale ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 Allison female white 0 3 1 TRUE ## 2 Kristen female white 0 3 2 TRUE ## 3 Lakisha female black 0 3 3 TRUE ## 4 Latonya female black 0 3 4 TRUE ## 5 Carrie female white 0 3 5 TRUE ## 6 Jay male white 0 3 6 FALSE Note that mutate() needs a dataframe as its first argument. Above, we pass df_resume to mutate() using the pipe. A.5.4 Create a sum of from subset of variables Below, we select a subset of columns from df_resume, take the sum across them, and add the sum as a new variable. Here, we locate the new variable right after those columns that it summed. df_resume %&gt;% mutate(new_sum = rowSums(select(df_resume, call:id_var))) %&gt;% relocate(new_sum, .after = id_var) ## # A tibble: 4,870 × 8 ## firstname sex race call wave id_var new_sum isFemale ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Allison female white 0 3 1 4 TRUE ## 2 Kristen female white 0 3 2 5 TRUE ## 3 Lakisha female black 0 3 3 6 TRUE ## 4 Latonya female black 0 3 4 7 TRUE ## 5 Carrie female white 0 3 5 8 TRUE ## 6 Jay male white 0 3 6 9 FALSE ## 7 Jill female white 0 3 7 10 TRUE ## 8 Kenya female black 0 3 8 11 TRUE ## 9 Latonya female black 0 3 9 12 TRUE ## 10 Tyrone male black 0 3 10 13 FALSE ## # … with 4,860 more rows A.6 Recode a variable’s values Instead of creating a new variable as above, we can use mutate() and case_when() to recode a variable. Suppose we wanted to code race: df_resume %&gt;% mutate( race = case_when( race == &quot;white&quot; ~ &quot;race_wh&quot;, # recode &quot;white&quot; to &quot;race_wh&quot; race == &quot;black&quot; ~ &quot;race_bl&quot;, # recode &quot;black&quot; to &quot;race_bl&quot; race == &quot;missing&quot; ~ NA_character_, # recode &quot;missing&quot; to (character) NA TRUE ~ race # recode any other value to original value of race ) ) ## # A tibble: 4,870 × 7 ## firstname sex race call wave id_var isFemale ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 Allison female race_wh 0 3 1 TRUE ## 2 Kristen female race_wh 0 3 2 TRUE ## 3 Lakisha female race_bl 0 3 3 TRUE ## 4 Latonya female race_bl 0 3 4 TRUE ## 5 Carrie female race_wh 0 3 5 TRUE ## 6 Jay male race_wh 0 3 6 FALSE ## 7 Jill female race_wh 0 3 7 TRUE ## 8 Kenya female race_bl 0 3 8 TRUE ## 9 Latonya female race_bl 0 3 9 TRUE ## 10 Tyrone male race_bl 0 3 10 FALSE ## # … with 4,860 more rows There is also recode(), but its lifecycle status is “questioning”: df_resume %&gt;% mutate( race = recode(race, &quot;white&quot; = 0, &quot;black&quot; = 1 ) ) ## # A tibble: 4,870 × 7 ## firstname sex race call wave id_var isFemale ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 Allison female 0 0 3 1 TRUE ## 2 Kristen female 0 0 3 2 TRUE ## 3 Lakisha female 1 0 3 3 TRUE ## 4 Latonya female 1 0 3 4 TRUE ## 5 Carrie female 0 0 3 5 TRUE ## 6 Jay male 0 0 3 6 FALSE ## 7 Jill female 0 0 3 7 TRUE ## 8 Kenya female 1 0 3 8 TRUE ## 9 Latonya female 1 0 3 9 TRUE ## 10 Tyrone male 1 0 3 10 FALSE ## # … with 4,860 more rows A.7 Treat dates as dates When we have dates stored as character strings, we transform them to dates. First, simulate some date data: # Simulate date data: df_resume &lt;- df_resume %&gt;% mutate( dob = paste0(sample(1:12, n(), replace = TRUE), &quot;/&quot;, sample(1:28, n(), replace = TRUE), &quot;/&quot;, sample(1900:2000, n(), replace = TRUE)) ) head(df_resume, 3) ## # A tibble: 3 × 8 ## firstname sex race call wave id_var isFemale dob ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 Allison female white 0 3 1 TRUE 11/21/1951 ## 2 Kristen female white 0 3 2 TRUE 2/21/1970 ## 3 Lakisha female black 0 3 3 TRUE 10/10/1994 The tidyverse includes the lubridate package. If you know the format of the character strings, there are many shortcut parsers like mdy(). Below, we recode character dates to be of date type: df_resume %&gt;% mutate( dob_date = mdy(dob) ) ## # A tibble: 4,870 × 9 ## firstname sex race call wave id_var isFemale dob dob_date ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;date&gt; ## 1 Allison female white 0 3 1 TRUE 11/21/1951 1951-11-21 ## 2 Kristen female white 0 3 2 TRUE 2/21/1970 1970-02-21 ## 3 Lakisha female black 0 3 3 TRUE 10/10/1994 1994-10-10 ## 4 Latonya female black 0 3 4 TRUE 4/20/1910 1910-04-20 ## 5 Carrie female white 0 3 5 TRUE 4/1/1980 1980-04-01 ## 6 Jay male white 0 3 6 FALSE 9/15/1970 1970-09-15 ## 7 Jill female white 0 3 7 TRUE 8/5/1956 1956-08-05 ## 8 Kenya female black 0 3 8 TRUE 4/20/1965 1965-04-20 ## 9 Latonya female black 0 3 9 TRUE 6/20/1935 1935-06-20 ## 10 Tyrone male black 0 3 10 FALSE 4/13/1900 1900-04-13 ## # … with 4,860 more rows If we aren’t sure of the format, or there are several, we use parse_date_time() and specify several “orders” of date format. parse() functions do their best to use the information in “orders”. Below, \"dmy\" alone would recode only the ambiguous dates, assuming they were in day-month-year order. Adding \"%m/%d/%Y\" instructs parse() to look for month/day/year formats. Finding many that are unambiguously that format, it assumes that format for the new variable. df_resume %&gt;% mutate( dob_date = as_date( parse_date_time(dob, orders = c(&quot;dmy&quot;, &quot;%m/%d/%Y&quot;)) ) ) ## # A tibble: 4,870 × 9 ## firstname sex race call wave id_var isFemale dob dob_date ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;date&gt; ## 1 Allison female white 0 3 1 TRUE 11/21/1951 1951-11-21 ## 2 Kristen female white 0 3 2 TRUE 2/21/1970 1970-02-21 ## 3 Lakisha female black 0 3 3 TRUE 10/10/1994 1994-10-10 ## 4 Latonya female black 0 3 4 TRUE 4/20/1910 1910-04-20 ## 5 Carrie female white 0 3 5 TRUE 4/1/1980 1980-04-01 ## 6 Jay male white 0 3 6 FALSE 9/15/1970 1970-09-15 ## 7 Jill female white 0 3 7 TRUE 8/5/1956 1956-08-05 ## 8 Kenya female black 0 3 8 TRUE 4/20/1965 1965-04-20 ## 9 Latonya female black 0 3 9 TRUE 6/20/1935 1935-06-20 ## 10 Tyrone male black 0 3 10 FALSE 4/13/1900 1900-04-13 ## # … with 4,860 more rows A.7.1 Make an age variable To make an age variable from the year component, df_resume %&gt;% mutate( age = 2023 - year(dob_date) ) ## # A tibble: 4,870 × 10 ## firstname sex race call wave id_var isFemale dob dob_date age ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Allison female white 0 3 1 TRUE 11/21/19… 1951-11-21 72 ## 2 Kristen female white 0 3 2 TRUE 2/21/1970 1970-02-21 53 ## 3 Lakisha female black 0 3 3 TRUE 10/10/19… 1994-10-10 29 ## 4 Latonya female black 0 3 4 TRUE 4/20/1910 1910-04-20 113 ## 5 Carrie female white 0 3 5 TRUE 4/1/1980 1980-04-01 43 ## 6 Jay male white 0 3 6 FALSE 9/15/1970 1970-09-15 53 ## 7 Jill female white 0 3 7 TRUE 8/5/1956 1956-08-05 67 ## 8 Kenya female black 0 3 8 TRUE 4/20/1965 1965-04-20 58 ## 9 Latonya female black 0 3 9 TRUE 6/20/1935 1935-06-20 88 ## 10 Tyrone male black 0 3 10 FALSE 4/13/1900 1900-04-13 123 ## # … with 4,860 more rows For more precision, the participants’ age today, df_resume %&gt;% mutate( age = as.numeric( interval(dob_date, today()), &quot;years&quot;) ) ## # A tibble: 4,870 × 10 ## firstname sex race call wave id_var isFemale dob dob_date age ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Allison female white 0 3 1 TRUE 11/21/19… 1951-11-21 71.3 ## 2 Kristen female white 0 3 2 TRUE 2/21/1970 1970-02-21 53.0 ## 3 Lakisha female black 0 3 3 TRUE 10/10/19… 1994-10-10 28.4 ## 4 Latonya female black 0 3 4 TRUE 4/20/1910 1910-04-20 113. ## 5 Carrie female white 0 3 5 TRUE 4/1/1980 1980-04-01 42.9 ## 6 Jay male white 0 3 6 FALSE 9/15/1970 1970-09-15 52.4 ## 7 Jill female white 0 3 7 TRUE 8/5/1956 1956-08-05 66.5 ## 8 Kenya female black 0 3 8 TRUE 4/20/1965 1965-04-20 57.8 ## 9 Latonya female black 0 3 9 TRUE 6/20/1935 1935-06-20 87.7 ## 10 Tyrone male black 0 3 10 FALSE 4/13/1900 1900-04-13 123. ## # … with 4,860 more rows A.7.2 Make a month-count variable We may have dates accurate to the day, but we want a count of which month of participation it is for each participant. Assume that each firstname in the resume data represents a participant, and each dob_date represents an event for that participant that we care about. We want to produce a variable that is 0 for the first month of participation (say, February), and 2 for the month two months later (April), regardless of whether the month between (March) appears in the data, and even if participants start or finish participation at very different times.8 Below, we group by firstname, then round all months to the first date of the month, then count those months (within firstname). We show the first three rows for each participant. df_resume %&gt;% # Ensure ranking _within_ firstname: group_by(firstname) %&gt;% # Round all months to first date of the month, then count: mutate(dob_month = floor_date(dob_date, unit = &quot;month&quot;), dob_month_count = (min(dob_month) %--% dob_month) %/% months(1) ) %&gt;% # For presentation, select, arrange, and slice: select(firstname, dob, dob_date, dob_month, dob_month_count) %&gt;% arrange(firstname, dob_month_count) %&gt;% slice(1:3) ## # A tibble: 108 × 5 ## # Groups: firstname [36] ## firstname dob dob_date dob_month dob_month_count ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Aisha 6/21/1902 1902-06-21 1902-06-01 0 ## 2 Aisha 9/23/1902 1902-09-23 1902-09-01 3 ## 3 Aisha 6/10/1903 1903-06-10 1903-06-01 12 ## 4 Allison 1/10/1900 1900-01-10 1900-01-01 0 ## 5 Allison 7/16/1900 1900-07-16 1900-07-01 6 ## 6 Allison 3/3/1902 1902-03-03 1902-03-01 26 ## 7 Anne 7/16/1900 1900-07-16 1900-07-01 0 ## 8 Anne 8/21/1900 1900-08-21 1900-08-01 1 ## 9 Anne 2/15/1901 1901-02-15 1901-02-01 7 ## 10 Brad 1/28/1900 1900-01-28 1900-01-01 0 ## # … with 98 more rows If we prefer the first month to be labeled 1 rather than 0, we just replace %/% months(1) with %/% months(1) + 1. A.8 Create a completely randomised indicator This uses mutate(), along with the sample() function to create a random vector of Treatment’s and Control’s and attach it to df_resume: set.seed(317334706) df_resume &lt;- df_resume %&gt;% mutate( my_new_treatment_assg = sample(c(&quot;Treatment&quot;, &quot;Control&quot;), size = n(), replace = TRUE)) head(df_resume %&gt;% select(-isFemale, -dob)) ## # A tibble: 6 × 8 ## firstname sex race call wave id_var dob_date my_new_treatment_assg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; &lt;chr&gt; ## 1 Allison female white 0 3 1 1951-11-21 Treatment ## 2 Kristen female white 0 3 2 1970-02-21 Control ## 3 Lakisha female black 0 3 3 1994-10-10 Treatment ## 4 Latonya female black 0 3 4 1910-04-20 Treatment ## 5 Carrie female white 0 3 5 1980-04-01 Control ## 6 Jay male white 0 3 6 1970-09-15 Treatment A.9 Plot a variable Use ggplot() to plot the distribution of calls back that these résumés receive. Calls are fairly rare. df_resume %&gt;% ggplot(aes(x = call)) + geom_bar() Look at the distribution by the putative race on the resume: df_resume %&gt;% ggplot(aes(x = call, fill = race)) + geom_bar(position = &quot;dodge2&quot;) Since there are exactly the same number of résumés with each race, these count distributions are comparable. A.10 Calculate a data summary A.10.1 Proportions for a categorical variable df_resume %&gt;% count(firstname) %&gt;% mutate(proportion = n / sum(n)) ## # A tibble: 36 × 3 ## firstname n proportion ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Aisha 180 0.0370 ## 2 Allison 232 0.0476 ## 3 Anne 242 0.0497 ## 4 Brad 63 0.0129 ## 5 Brendan 65 0.0133 ## 6 Brett 59 0.0121 ## 7 Carrie 168 0.0345 ## 8 Darnell 42 0.00862 ## 9 Ebony 208 0.0427 ## 10 Emily 227 0.0466 ## # … with 26 more rows A.10.2 Proportions for binary variables summ_resume &lt;- df_resume %&gt;% group_by(race) %&gt;% summarise(call_back_rate = mean(call), # mean of numeric 0/1 prop_female = mean(sex == &quot;female&quot;)) # mean of logical TRUE/FALSE summ_resume ## # A tibble: 2 × 3 ## race call_back_rate prop_female ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 black 0.0645 0.775 ## 2 white 0.0965 0.764 The call-back rates differ by about 0.032; the proportion female, which will be balanced by the randomization on average, only differs by about 0.011. A.11 Merge (join) dataframes For a fuller treatment of merges (“joins”), see here in Wickham and Grolemund (2016). The four main mutating join commands are left_join(), right_join(), inner_join(), and full_join(). left_join(x, y) keeps all rows of x, but only rows of y that have matches in x right_join(x, y) keeps all rows of y, but only rows of x that have matches in y inner_join(x, y) only keeps rows that appear in both x andy full_join(x, y) keeps all rows of both x and y For any columns that didn’t appear in the original dataframe for a certain row, the joins will store NA. By default, joins will match on any columns that have the same name. So, if age and height are both in df1 and df2, then full_join(df1, df2) will consider a row “matched” if the values of both age and height are the same in the datasets. If column names differ, the by = ... argument can inform the join. E.g., if, instead, df1 has variables Age and Tallness, we could join with df_joined &lt;- df1 %&gt;% full_join(df2, by = c(&quot;Age&quot; = &quot;age&quot;, &quot;Tallness&quot; = &quot;height&quot;)) Columns with the same name, but different types, will not join. E.g., if df1$this_var is numeric, but df2$this_var is character, the joins will throw an error. References "],["python-basics-using-rrstudio.html", "B Python Basics (using R/RStudio) B.1 Installing tools, R packages, and Python B.2 Using Python", " B Python Basics (using R/RStudio) B.1 Installing tools, R packages, and Python Posit provides instructions for using Python from RStudio (including .py files, and Python and R in the same document) at https://support.posit.co/hc/en-us/articles/1500007929061-Using-Python-with-the-RStudio-IDE B.1.1 Get Rtools You will need some developer tools. If you are on Windows, download the installer and click the .exe to install the latest version of Rtools from https://cran.rstudio.com/bin/windows/Rtools/rtools43/rtools.html B.1.2 Get tinytex Install the tinytex package (and tinytex itself) at the R Console via install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() B.1.3 Get Python Download and install Python from https://www.python.org/downloads/ B.1.4 Configure Python Via, e.g., ./configure --enable-shared make make install B.1.5 Get reticulate Install the reticulate package at the R Console via install.packages(&quot;reticulate&quot;) B.2 Using Python B.2.1 From a Python prompt reticulate::repl_python() B.2.2 From inside a Quarto document "],["managing-code.html", "C Managing Code C.1 Best Practices C.2 Style C.3 Projects C.4 Working Directory and Relative Paths C.5 What Packages are Installed?", " C Managing Code C.1 Best Practices Use “R projects” and the here package to manage the working directory and paths. See below in C.3 and C.4 for some details. This is how we promote replication and collaboration (including with our future selves). Use high-quality names for objects, files, and directories, like df_resume, 03-analysis.R, and /code/ (not finalTHING or more stuff.R or /Next Try/). Names should be human- and machine-readable. File names should start with numbers, so that default ordering is correct, as in 00-prelims.R, 01-power-calcs.R, etc., be informative, not have spaces, and separate words with - or _ to enable human reading and globbing. Object and directory names also should be informative, not have spaces, and use _ to separate words. See these slides for more detail and painful counterexamples. Use GitHub. But do not put sensitive data there. Begin your code file with a library() command for every package required for that file to run. Keep these in alphabetical order. library(here) library(tidyverse) Do not include your “workflow” in your code files. Anything that’s interactive or changes the environment should be excluded. Do not include install.packages(), View(), setwd(), in your .R file. Do not start your .R file with rm(list = ls()); it is both too strong and too weak.9 Don’t open or change the original data. Read it in programmatically with, e.g., read_csv() or read_excel(). See here for examples in R. Use the assignment arrow for assignment. The structure: new_obj_name &lt;- value_of_that_object Use space around operators (write 3 + 4 = 7, not 3+4=7) and after a comma, as in English (write f(x, y), not f(x,y)). C.2 Style When we write in R, we tend to prefer the tidyverse style. See https://style.tidyverse.org for the full treatment with many examples. Of course, there is a package, styler, with functions that will style your code for you. See here for more detail. C.3 Projects Create a .Rproj file in your project’s top-level directory, making your project an “R project”. In RStudio, File - New Project - Existing Directory (or New Directory, if no project folder exists). Then, to start work, always open the .Rproj file. This ensures that you have a fresh instance of R and RStudio, and the working directory is always the same. The working directory is the top-level directory (that is, the directory within which the .Rproj file lives). C.4 Working Directory and Relative Paths The “working directory” is the directory where R will look for data, code files, etc. and save your output objects (a new .csv or a plot .pdf) by default. Opening the .Rproj file ensures that the working directory always starts at the top-level directory of your project. We create paths to our objects using the here package. This ensures that a) we are not hard-coding a path that no one else has (like ~/Me/My Docs/my_special_folder/my_subfolder/, etc.), and b) our code is platform-independent. For more on the here package, see here. The code below requires the following packages to be loaded and attached: library(here) C.4.1 See the working directory To see the current working directory, type getwd(). C.4.2 See the project directory The “project directory” is the top-level directory of your project. It should be the working directory, as well, if you follow the advice at C.3, and start work by opening the .Rproj file. To see the project directory: here() ## [1] &quot;/Users/ryanmoore/Documents/github/thelab/LAB-SOP-experiments&quot; C.4.3 Create a path with here() I have an object called 02-01-df.RData in a subdirectory called /data/. The dir() function shows what is in that subdirectory: dir(&quot;data&quot;) ## [1] &quot;02-01-df.RData&quot; To see its full path, here(&quot;data&quot;, &quot;02-01-df.RData&quot;) ## [1] &quot;/Users/ryanmoore/Documents/github/thelab/LAB-SOP-experiments/data/02-01-df.RData&quot; To use that path to read in the data, I first create the path, then use it to read in the object: my_rdata_path &lt;- here(&quot;data&quot;, &quot;02-01-df.RData&quot;) # Use load() for an .RData object: load(my_rdata_path) (These could be combined into a single line.) The object 02-01-df.RData contains a single dataframe called df. I can now see that dataframe in my environment: ls() ## [1] &quot;df&quot; &quot;my_rdata_path&quot; and examine it head(df) ## x z y ## 1 5.8873201 0 5.6125511 ## 2 2.3428286 0 2.0830483 ## 3 3.9439262 0 2.5861546 ## 4 3.3373187 1 2.6527890 ## 5 2.7946122 0 0.6784908 ## 6 0.6943772 0 2.8216648 C.5 What Packages are Installed? To see what packages are installed, library() or lapply(.libPaths(), dir) ## [[1]] ## [1] &quot;abind&quot; &quot;askpass&quot; &quot;assertthat&quot; &quot;backports&quot; ## [5] &quot;base&quot; &quot;base64enc&quot; &quot;bit&quot; &quot;bit64&quot; ## [9] &quot;blob&quot; &quot;blockTools&quot; &quot;bookdown&quot; &quot;boot&quot; ## [13] &quot;brew&quot; &quot;brio&quot; &quot;broom&quot; &quot;bslib&quot; ## [17] &quot;cachem&quot; &quot;callr&quot; &quot;car&quot; &quot;carData&quot; ## [21] &quot;cellranger&quot; &quot;checkmate&quot; &quot;class&quot; &quot;cli&quot; ## [25] &quot;clipr&quot; &quot;cluster&quot; &quot;codetools&quot; &quot;colorspace&quot; ## [29] &quot;commonmark&quot; &quot;compiler&quot; &quot;cpp11&quot; &quot;crayon&quot; ## [33] &quot;credentials&quot; &quot;crosstalk&quot; &quot;curl&quot; &quot;data.table&quot; ## [37] &quot;datasets&quot; &quot;DBI&quot; &quot;dbplyr&quot; &quot;desc&quot; ## [41] &quot;devtools&quot; &quot;diffobj&quot; &quot;digest&quot; &quot;directlabels&quot; ## [45] &quot;downlit&quot; &quot;dplyr&quot; &quot;DT&quot; &quot;dtplyr&quot; ## [49] &quot;ellipsis&quot; &quot;estimatr&quot; &quot;evaluate&quot; &quot;fabricatr&quot; ## [53] &quot;fansi&quot; &quot;farver&quot; &quot;fastmap&quot; &quot;fontawesome&quot; ## [57] &quot;forcats&quot; &quot;foreach&quot; &quot;foreign&quot; &quot;Formula&quot; ## [61] &quot;formula.tools&quot; &quot;fs&quot; &quot;gargle&quot; &quot;generics&quot; ## [65] &quot;gert&quot; &quot;ggplot2&quot; &quot;gh&quot; &quot;gitcreds&quot; ## [69] &quot;glue&quot; &quot;googledrive&quot; &quot;googlesheets4&quot; &quot;graphics&quot; ## [73] &quot;grDevices&quot; &quot;grid&quot; &quot;gridExtra&quot; &quot;gtable&quot; ## [77] &quot;haven&quot; &quot;here&quot; &quot;highr&quot; &quot;hms&quot; ## [81] &quot;htmltools&quot; &quot;htmlwidgets&quot; &quot;httpuv&quot; &quot;httr&quot; ## [85] &quot;ids&quot; &quot;ini&quot; &quot;isoband&quot; &quot;iterators&quot; ## [89] &quot;janitor&quot; &quot;jquerylib&quot; &quot;jsonlite&quot; &quot;KernSmooth&quot; ## [93] &quot;knitr&quot; &quot;labeling&quot; &quot;later&quot; &quot;lattice&quot; ## [97] &quot;lazyeval&quot; &quot;lifecycle&quot; &quot;lme4&quot; &quot;lmtest&quot; ## [101] &quot;lubridate&quot; &quot;magrittr&quot; &quot;markdown&quot; &quot;MASS&quot; ## [105] &quot;Matrix&quot; &quot;MatrixModels&quot; &quot;memoise&quot; &quot;methods&quot; ## [109] &quot;metR&quot; &quot;mgcv&quot; &quot;mime&quot; &quot;miniUI&quot; ## [113] &quot;minqa&quot; &quot;modelr&quot; &quot;multitestr&quot; &quot;munsell&quot; ## [117] &quot;nlme&quot; &quot;nloptr&quot; &quot;nnet&quot; &quot;numDeriv&quot; ## [121] &quot;openssl&quot; &quot;operator.tools&quot; &quot;parallel&quot; &quot;pbkrtest&quot; ## [125] &quot;pillar&quot; &quot;pkgbuild&quot; &quot;pkgconfig&quot; &quot;pkgdown&quot; ## [129] &quot;pkgload&quot; &quot;plyr&quot; &quot;png&quot; &quot;praise&quot; ## [133] &quot;prettyunits&quot; &quot;processx&quot; &quot;profvis&quot; &quot;progress&quot; ## [137] &quot;promises&quot; &quot;ps&quot; &quot;purrr&quot; &quot;quadprog&quot; ## [141] &quot;quantreg&quot; &quot;R6&quot; &quot;ragg&quot; &quot;randomizr&quot; ## [145] &quot;rappdirs&quot; &quot;rcmdcheck&quot; &quot;RColorBrewer&quot; &quot;Rcpp&quot; ## [149] &quot;RcppEigen&quot; &quot;readr&quot; &quot;readxl&quot; &quot;rematch&quot; ## [153] &quot;rematch2&quot; &quot;remotes&quot; &quot;renv&quot; &quot;reprex&quot; ## [157] &quot;rJava&quot; &quot;rlang&quot; &quot;rmarkdown&quot; &quot;roxygen2&quot; ## [161] &quot;rpart&quot; &quot;rprojroot&quot; &quot;rstudioapi&quot; &quot;rversions&quot; ## [165] &quot;rvest&quot; &quot;sandwich&quot; &quot;sass&quot; &quot;scales&quot; ## [169] &quot;selectr&quot; &quot;sessioninfo&quot; &quot;shiny&quot; &quot;snakecase&quot; ## [173] &quot;sourcetools&quot; &quot;sp&quot; &quot;SparseM&quot; &quot;spatial&quot; ## [177] &quot;splines&quot; &quot;stats&quot; &quot;stats4&quot; &quot;stringi&quot; ## [181] &quot;stringr&quot; &quot;survival&quot; &quot;sys&quot; &quot;systemfonts&quot; ## [185] &quot;tcltk&quot; &quot;testthat&quot; &quot;textshaping&quot; &quot;tibble&quot; ## [189] &quot;tidyr&quot; &quot;tidyselect&quot; &quot;tidyverse&quot; &quot;timechange&quot; ## [193] &quot;tinytex&quot; &quot;tools&quot; &quot;translations&quot; &quot;tzdb&quot; ## [197] &quot;urlchecker&quot; &quot;usethis&quot; &quot;utf8&quot; &quot;utils&quot; ## [201] &quot;uuid&quot; &quot;vctrs&quot; &quot;viridisLite&quot; &quot;vroom&quot; ## [205] &quot;waldo&quot; &quot;whisker&quot; &quot;withr&quot; &quot;xfun&quot; ## [209] &quot;xlsxjars&quot; &quot;xml2&quot; &quot;xopen&quot; &quot;xtable&quot; ## [213] &quot;yaml&quot; &quot;zip&quot; &quot;zoo&quot; It is “too weak” in that you should start a new R session regularly to ensure replicability. Removing objects from the workspace does nothing to packages, session options, graphical par()’s, etc.↩︎ "],["getting-started-with-git-and-github.html", "D Getting Started with git and GitHub", " D Getting Started with git and GitHub We use the git version control system and the GitHub web interface to manage version control. To get started, follow the steps outlined by the chapters in Bryan (2022). In particular, Register a GitHub account (The Lab @ DC requests that you make a separate account from any person GitHub account you may have.) Update R and RStudio (if you will use R, or use RStudio for interacting with git regardless of whether you use R) Install git Introduce yourself to git You are basically ready, but press on. First, simplify interacting with git, so that you don’t have to enter your password every time you make a commit or fetch remote code. Set up a PAT or SSH key Then, practice making a repo in the chapter “Connect to GitHub”. At The Lab, we do not store sensitive data on GitHub. We have a few template repositories: The data science team’s current template A simple, not-organized-by-project-phase one here An older (out of date?) one References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
